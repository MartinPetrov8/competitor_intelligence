# Competitor Tracker Setup — Progress

## Branch Status
- Branch: feature/competitor-tracker-mvp
- Base: main (initial commit)
- Status: Ready for development

## Build & Test Configuration
**Status:** Not yet configured (fresh repository)

**Recommended Setup:**
- Create requirements.txt with: requests, beautifulsoup4, flask (for dashboard), pytest (for tests)
- Create Makefile or scripts for common tasks
- Add pytest.ini for test configuration

## Project Hygiene
✅ .gitignore created (Python + SQLite + environment files)
✅ .env.example created (dashboard, database, scraper config placeholders)

## Baseline Status
- No existing code to build or test
- Clean slate — no pre-existing failures
- Git repository initialized with main branch

## Codebase Patterns

Since this is a fresh repository, here are the **recommended patterns** the developer should follow based on the project requirements:

### Project Structure
```
/scrapers/           # Individual scraper modules
  pricing.py         # Pricing scraper
  products.py        # Product catalog scraper
  snapshots.py       # Website snapshot scraper
  reviews.py         # Review monitoring (Trustpilot + Google)
/database/           # Database schema and utilities
  schema.py          # Table definitions
  db_utils.py        # DB connection and common queries
/dashboard/          # HTML dashboard
  app.py             # Flask server (port 3001)
  static/            # CSS, JS, Chart.js
  templates/         # HTML templates
run_daily.py         # Main runner script
requirements.txt     # Python dependencies
README.md            # Setup and cron instructions
.env                 # Local config (gitignored)
.env.example         # Template for environment variables
```

### Error Handling Pattern
**CRITICAL:** Each scraper must handle errors gracefully without crashing the entire run.

```python
# Pattern: try/except with logging, continue on failure
import logging

def scrape_competitor(competitor_url):
    try:
        # scraping logic
        response = requests.get(competitor_url, timeout=30)
        response.raise_for_status()
        # parse and store
        return True
    except requests.Timeout:
        logging.error(f"Timeout scraping {competitor_url}")
        return False
    except requests.RequestException as e:
        logging.error(f"Error scraping {competitor_url}: {e}")
        return False
    except Exception as e:
        logging.error(f"Unexpected error scraping {competitor_url}: {e}")
        return False
```

### Database Pattern
**Use context managers for SQLite connections:**

```python
import sqlite3
from contextlib import contextmanager

@contextmanager
def get_db_connection(db_path):
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row  # Access columns by name
    try:
        yield conn
        conn.commit()
    except Exception:
        conn.rollback()
        raise
    finally:
        conn.close()
```

### Daily Runner Pattern (run_daily.py)
**Sequential execution with failure isolation:**

```python
# Each scraper runs independently
# One failure doesn't stop the others
# Log all results

scrapers = [
    ("Pricing", scrape_pricing),
    ("Products", scrape_products),
    ("Snapshots", scrape_snapshots),
    ("Reviews", scrape_reviews),
]

results = {}
for name, scraper_func in scrapers:
    try:
        logging.info(f"Running {name} scraper...")
        success = scraper_func()
        results[name] = "SUCCESS" if success else "FAILED"
    except Exception as e:
        logging.error(f"{name} scraper crashed: {e}")
        results[name] = "CRASHED"

# Exit 0 even if some scrapers failed (graceful degradation)
sys.exit(0)
```

### Testing Pattern
**File naming:** `test_<module>.py` in same directory or `/tests` directory
**Fallback:** Keep core tests compatible with stdlib `unittest` so `python3 -m unittest` works in constrained environments.
**Use pytest fixtures for database setup:**

```python
import pytest
import sqlite3

@pytest.fixture
def test_db():
    conn = sqlite3.connect(":memory:")
    # Create schema
    # ...
    yield conn
    conn.close()

def test_pricing_scraper(test_db):
    # Test with mock data
    assert scraper_function() is not None
```

### Dashboard Requirements
- **Port:** 3001 (configurable via .env)
- **Auth:** Basic password protection (shared credential)
- **Response format:** HTML with embedded Chart.js
- **Date picker:** Allow viewing historical data
- **Charts:** Line charts for trends (pricing, review counts)
- **Diff viewer:** Side-by-side or inline diff with highlighting

### Key Conventions
- **Naming:** snake_case for Python files and functions
- **Logging:** Use Python's logging module (not print statements)
- **Config:** Load from .env using python-dotenv or os.environ
- **Database:** Single SQLite file (competitor_data.db)
- **Timestamps:** Store as ISO 8601 strings or Unix timestamps
- **Competitor list:** Hardcoded in scrapers or config file
- **Scrape deduplication:** When scraping multiple paths for one competitor, deduplicate by stable semantic keys before DB insert.
- **Snapshot diffs:** Normalize HTML with stripped text nodes and timestamp tokenization before `difflib.unified_diff` to reduce noisy whitespace/timestamp-only diffs.

### Definition of Done Checklist
The developer must ensure:
- [ ] `python run_daily.py` exits 0 and populates all DB tables
- [ ] Dashboard loads at localhost:3001 with data for all 5 competitors
- [ ] Price data matches manual spot-check for at least 3 competitors
- [ ] Review counts match Trustpilot/Google within ±2
- [ ] Website diff correctly highlights changes between snapshots
- [ ] Scraper handles timeouts/blocks without crashing (logs error, continues)
- [ ] Dashboard viewable by anyone with the URL (no local setup needed)
- [ ] README with setup and cron instructions

## 2026-02-22 23:13 UTC - S-1: Project scaffolding and SQLite schema
- Implemented initial project scaffolding directories: `database/`, `scrapers/`, `dashboard/` (with `static/` and `templates/`), and `tests/`.
- Added `requirements.txt` with core scraper/parser dependencies (`requests`, `beautifulsoup4`, `lxml`) and project tooling (`flask`, `python-dotenv`, `pytest`, `mypy`).
- Added `database/schema.py` with table DDL for: `competitors`, `prices`, `products`, `snapshots`, `diffs`, `reviews_trustpilot`, `reviews_google`, and `ab_tests`.
- Added `init_db.py` migration/init entrypoint to create tables/indexes and seed the 5 required competitors.
- Added tests in `tests/test_init_db.py` to validate table creation, competitor seeding, and date+competitor indexes.
- **Learnings:** For this environment, standard-library `unittest` and `compileall` are reliable validation fallbacks when third-party tooling is not preinstalled.
---

## 2026-02-22 23:21 UTC - S-2: Pricing scraper module
- Implemented `scrapers/pricing.py` with competitor iteration, multi-path pricing page fetches, robust timeout/HTTP error handling, and SQLite inserts into `prices`.
- Added parser logic to extract product names, currency, price amount, and bundle text from HTML using BeautifulSoup.
- Added schema-safe write behavior to ensure `price_usd` column exists and is populated alongside `price_amount`.
- Added tests in `tests/test_pricing_scraper.py` covering HTML extraction, timeout handling, HTTP error handling, and DB persistence requirements.
- Updated mypy config for `requests` third-party typing availability in this environment.
- **Learnings:** Competitor pages commonly expose price info on the homepage when dedicated `/pricing` routes return 404, so scraper checks both base URL and likely pricing paths.
---

## 2026-02-22 23:40 UTC - S-3: Product catalog scraper module
- Implemented `scrapers/products.py` with resilient multi-path product scraping across all competitors and graceful timeout/HTTP error handling.
- Added HTML parsing to extract `product_name`, `description`, `price_range`, `url`, and timestamps; writes to `products` table with schema-safe migrations for `price_range` and `url` columns.
- Added historical new-product detection by comparing current scrape against prior-day product names per competitor.
- Added per-competitor deduplication across multiple scraped pages to avoid duplicate product inserts.
- Added tests in `tests/test_products_scraper.py` for parser correctness, persistence field coverage, timeout/error continuation behavior, and historical new-product detection workflow.
- **Learnings:** Matching domain substrings in scraper tests can cause false positives (`onwardticket.com` vs `bestonwardticket.com`), so tests should use ordered/specific host checks.
---

## 2026-02-22 23:46 UTC - S-4: Website snapshot and diff module
- Implemented `scrapers/snapshots.py` to capture homepage + pricing-page HTML per competitor, persist raw snapshots, and generate/store unified diffs against prior snapshots.
- Added diff normalization logic to ignore pure whitespace changes and dynamic timestamp strings before comparison.
- Added resilience patterns consistent with prior scrapers (timeouts/HTTP errors logged per-competitor; scraper continues and returns successfully when partial data is available).
- Added tests in `tests/test_snapshots_scraper.py` for normalization behavior and end-to-end snapshot/diff persistence across consecutive runs.
- **Learnings:** Using `BeautifulSoup(...).stripped_strings` as the diff normalization basis provides stable textual diffs while filtering markup-format noise.
- **SKILLS_APPLIED:** web-scraping (requests, BeautifulSoup, difflib for HTML snapshot capture and normalized diff generation)
---

## 2026-02-22 23:57 UTC - S-5: Trustpilot review scraper
- Implemented `scrapers/reviews_trustpilot.py` with competitor iteration against Trustpilot review pages, JSON-LD parsing for aggregate rating/review count, and star-distribution extraction from embedded data/fallback page text.
- Added schema-safe migrations in scraper for `review_count` and `stars_5` through `stars_1` columns while maintaining compatibility with existing `total_reviews`/`rating_*` columns.
- Added graceful handling for missing/blocked pages (timeouts and HTTP errors logged, scraper continues without crashing).
- Added tests in `tests/test_reviews_trustpilot.py` covering parser extraction and end-to-end persistence behavior with partial failures.
- **Learnings:** Trustpilot metric fields are most reliably exposed through `application/ld+json` aggregate data; star breakdown often appears in non-JSON script blobs requiring regex extraction.
- **SKILLS_APPLIED:** web-scraping (Trustpilot extraction via JSON-LD + fallback script/text parsing)
---
