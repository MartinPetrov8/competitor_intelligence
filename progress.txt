# Competitor Tracker Setup — Progress

## Branch Status
- Branch: feature/competitor-tracker-mvp
- Base: main (initial commit)
- Status: Ready for development

## Build & Test Configuration
**Status:** Not yet configured (fresh repository)

**Recommended Setup:**
- Create requirements.txt with: requests, beautifulsoup4, flask (for dashboard), pytest (for tests)
- Create Makefile or scripts for common tasks
- Add pytest.ini for test configuration

## Project Hygiene
✅ .gitignore created (Python + SQLite + environment files)
✅ .env.example created (dashboard, database, scraper config placeholders)

## Baseline Status
- No existing code to build or test
- Clean slate — no pre-existing failures
- Git repository initialized with main branch

## Codebase Patterns

Since this is a fresh repository, here are the **recommended patterns** the developer should follow based on the project requirements:

### Project Structure
```
/scrapers/           # Individual scraper modules
  pricing.py         # Pricing scraper
  products.py        # Product catalog scraper
  snapshots.py       # Website snapshot scraper
  reviews.py         # Review monitoring (Trustpilot + Google)
/database/           # Database schema and utilities
  schema.py          # Table definitions
  db_utils.py        # DB connection and common queries
/dashboard/          # HTML dashboard
  app.py             # Flask server (port 3001)
  static/            # CSS, JS, Chart.js
  templates/         # HTML templates
run_daily.py         # Main runner script
requirements.txt     # Python dependencies
README.md            # Setup and cron instructions
.env                 # Local config (gitignored)
.env.example         # Template for environment variables
```

### Error Handling Pattern
**CRITICAL:** Each scraper must handle errors gracefully without crashing the entire run.

```python
# Pattern: try/except with logging, continue on failure
import logging

def scrape_competitor(competitor_url):
    try:
        # scraping logic
        response = requests.get(competitor_url, timeout=30)
        response.raise_for_status()
        # parse and store
        return True
    except requests.Timeout:
        logging.error(f"Timeout scraping {competitor_url}")
        return False
    except requests.RequestException as e:
        logging.error(f"Error scraping {competitor_url}: {e}")
        return False
    except Exception as e:
        logging.error(f"Unexpected error scraping {competitor_url}: {e}")
        return False
```

### Database Pattern
**Use context managers for SQLite connections:**

```python
import sqlite3
from contextlib import contextmanager

@contextmanager
def get_db_connection(db_path):
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row  # Access columns by name
    try:
        yield conn
        conn.commit()
    except Exception:
        conn.rollback()
        raise
    finally:
        conn.close()
```

### Daily Runner Pattern (run_daily.py)
**Sequential execution with failure isolation:**

```python
# Each scraper runs independently
# One failure doesn't stop the others
# Log all results

scrapers = [
    ("Pricing", scrape_pricing),
    ("Products", scrape_products),
    ("Snapshots", scrape_snapshots),
    ("Reviews", scrape_reviews),
]

results = {}
for name, scraper_func in scrapers:
    try:
        logging.info(f"Running {name} scraper...")
        success = scraper_func()
        results[name] = "SUCCESS" if success else "FAILED"
    except Exception as e:
        logging.error(f"{name} scraper crashed: {e}")
        results[name] = "CRASHED"

# Exit 0 even if some scrapers failed (graceful degradation)
sys.exit(0)
```

### Testing Pattern
**File naming:** `test_<module>.py` in same directory or `/tests` directory
**Fallback:** Keep core tests compatible with stdlib `unittest` so `python3 -m unittest` works in constrained environments.
**Use pytest fixtures for database setup:**

```python
import pytest
import sqlite3

@pytest.fixture
def test_db():
    conn = sqlite3.connect(":memory:")
    # Create schema
    # ...
    yield conn
    conn.close()

def test_pricing_scraper(test_db):
    # Test with mock data
    assert scraper_function() is not None
```

### Dashboard Requirements
- **Port:** 3001 (configurable via .env)
- **Auth:** Basic password protection (shared credential)
- **Response format:** HTML with embedded Chart.js
- **Date picker:** Allow viewing historical data
- **Charts:** Line charts for trends (pricing, review counts)
- **Diff viewer:** Side-by-side or inline diff with highlighting

### Key Conventions
- **Naming:** snake_case for Python files and functions
- **Logging:** Use Python's logging module (not print statements)
- **Config:** Load from .env using python-dotenv or os.environ
- **Database:** Single SQLite file (competitor_data.db)
- **Timestamps:** Store as ISO 8601 strings or Unix timestamps
- **Competitor list:** Hardcoded in scrapers or config file
- **Scrape deduplication:** When scraping multiple paths for one competitor, deduplicate by stable semantic keys before DB insert.
- **Snapshot diffs:** Normalize HTML with stripped text nodes and timestamp tokenization before `difflib.unified_diff` to reduce noisy whitespace/timestamp-only diffs.
- **A/B detection:** Scan homepage HTML plus `<script src>` and inline script content; match tools using a signature map and persist only positive detections with compact evidence snippets.
- **Daily orchestration:** In `run_daily.py`, model each scraper as a named task with target DB tables; compute `rows_inserted` via before/after table counts to produce stable per-scraper summaries even when scraper APIs return only booleans.
- **Dashboard API pattern:** Keep a `create_app(db_path)` Flask app factory for testability, and centralize SQL/filter logic in a store class so all `/api/*` endpoints share identical date+competitor filtering semantics.

### Definition of Done Checklist
The developer must ensure:
- [ ] `python run_daily.py` exits 0 and populates all DB tables
- [ ] Dashboard loads at localhost:3001 with data for all 5 competitors
- [ ] Price data matches manual spot-check for at least 3 competitors
- [ ] Review counts match Trustpilot/Google within ±2
- [ ] Website diff correctly highlights changes between snapshots
- [ ] Scraper handles timeouts/blocks without crashing (logs error, continues)
- [ ] Dashboard viewable by anyone with the URL (no local setup needed)
- [ ] README with setup and cron instructions

## 2026-02-22 23:13 UTC - S-1: Project scaffolding and SQLite schema
- Implemented initial project scaffolding directories: `database/`, `scrapers/`, `dashboard/` (with `static/` and `templates/`), and `tests/`.
- Added `requirements.txt` with core scraper/parser dependencies (`requests`, `beautifulsoup4`, `lxml`) and project tooling (`flask`, `python-dotenv`, `pytest`, `mypy`).
- Added `database/schema.py` with table DDL for: `competitors`, `prices`, `products`, `snapshots`, `diffs`, `reviews_trustpilot`, `reviews_google`, and `ab_tests`.
- Added `init_db.py` migration/init entrypoint to create tables/indexes and seed the 5 required competitors.
- Added tests in `tests/test_init_db.py` to validate table creation, competitor seeding, and date+competitor indexes.
- **Learnings:** For this environment, standard-library `unittest` and `compileall` are reliable validation fallbacks when third-party tooling is not preinstalled.
---

## 2026-02-22 23:21 UTC - S-2: Pricing scraper module
- Implemented `scrapers/pricing.py` with competitor iteration, multi-path pricing page fetches, robust timeout/HTTP error handling, and SQLite inserts into `prices`.
- Added parser logic to extract product names, currency, price amount, and bundle text from HTML using BeautifulSoup.
- Added schema-safe write behavior to ensure `price_usd` column exists and is populated alongside `price_amount`.
- Added tests in `tests/test_pricing_scraper.py` covering HTML extraction, timeout handling, HTTP error handling, and DB persistence requirements.
- Updated mypy config for `requests` third-party typing availability in this environment.
- **Learnings:** Competitor pages commonly expose price info on the homepage when dedicated `/pricing` routes return 404, so scraper checks both base URL and likely pricing paths.
---

## 2026-02-22 23:40 UTC - S-3: Product catalog scraper module
- Implemented `scrapers/products.py` with resilient multi-path product scraping across all competitors and graceful timeout/HTTP error handling.
- Added HTML parsing to extract `product_name`, `description`, `price_range`, `url`, and timestamps; writes to `products` table with schema-safe migrations for `price_range` and `url` columns.
- Added historical new-product detection by comparing current scrape against prior-day product names per competitor.
- Added per-competitor deduplication across multiple scraped pages to avoid duplicate product inserts.
- Added tests in `tests/test_products_scraper.py` for parser correctness, persistence field coverage, timeout/error continuation behavior, and historical new-product detection workflow.
- **Learnings:** Matching domain substrings in scraper tests can cause false positives (`onwardticket.com` vs `bestonwardticket.com`), so tests should use ordered/specific host checks.
---

## 2026-02-22 23:46 UTC - S-4: Website snapshot and diff module
- Implemented `scrapers/snapshots.py` to capture homepage + pricing-page HTML per competitor, persist raw snapshots, and generate/store unified diffs against prior snapshots.
- Added diff normalization logic to ignore pure whitespace changes and dynamic timestamp strings before comparison.
- Added resilience patterns consistent with prior scrapers (timeouts/HTTP errors logged per-competitor; scraper continues and returns successfully when partial data is available).
- Added tests in `tests/test_snapshots_scraper.py` for normalization behavior and end-to-end snapshot/diff persistence across consecutive runs.
- **Learnings:** Using `BeautifulSoup(...).stripped_strings` as the diff normalization basis provides stable textual diffs while filtering markup-format noise.
- **SKILLS_APPLIED:** web-scraping (requests, BeautifulSoup, difflib for HTML snapshot capture and normalized diff generation)
---

## 2026-02-22 23:57 UTC - S-5: Trustpilot review scraper
- Implemented `scrapers/reviews_trustpilot.py` with competitor iteration against Trustpilot review pages, JSON-LD parsing for aggregate rating/review count, and star-distribution extraction from embedded data/fallback page text.
- Added schema-safe migrations in scraper for `review_count` and `stars_5` through `stars_1` columns while maintaining compatibility with existing `total_reviews`/`rating_*` columns.
- Added graceful handling for missing/blocked pages (timeouts and HTTP errors logged, scraper continues without crashing).
- Added tests in `tests/test_reviews_trustpilot.py` covering parser extraction and end-to-end persistence behavior with partial failures.
- **Learnings:** Trustpilot metric fields are most reliably exposed through `application/ld+json` aggregate data; star breakdown often appears in non-JSON script blobs requiring regex extraction.
- **SKILLS_APPLIED:** web-scraping (Trustpilot extraction via JSON-LD + fallback script/text parsing)
---

## 2026-02-23 00:02 UTC - S-6: Google Reviews scraper
- Implemented `scrapers/reviews_google.py` with competitor iteration, Google search-page fetches, JSON-LD + itemprop + text fallback parsing, and SQLite persistence into `reviews_google`.
- Added schema-safe migration for `review_count` while keeping compatibility with existing `total_reviews` column.
- Added resilient behavior for missing Google listings/timeouts/HTTP errors: logs warning and continues scraping remaining competitors.
- Added tests in `tests/test_reviews_google.py` for parser extraction and end-to-end DB persistence with partial failures.
- **Learnings:** Google review metrics are inconsistent across pages, so layered extraction (JSON-LD first, then semantic HTML tags, then text regex fallback) improves robustness.
- **SKILLS_APPLIED:** web-scraping (Google review extraction with structured + fallback parsers)
---

## 2026-02-23 00:08 UTC - S-7: A/B test detection module
- Implemented `scrapers/ab_tests.py` with homepage scanning for known A/B testing frameworks and robust timeout/HTTP error handling.
- Added signature detection for 7 frameworks: Optimizely, VWO, Google Optimize, LaunchDarkly, Adobe Target, Split, and Convert.
- Added persistence flow into `ab_tests` table (per competitor/date/tool) with evidence snippets for matched signatures.
- Added CLI entrypoint support so `python3 -m scrapers.ab_tests` runs safely and exits with proper status codes.
- Added tests in `tests/test_ab_tests_scraper.py` for multi-signature detection and end-to-end DB persistence with partial scrape failures.
- **Learnings:** A/B framework presence is most reliably detected by searching full HTML plus script `src` attributes and inline script text, then storing short context evidence to aid dashboard debugging.
- **SKILLS_APPLIED:** web-scraping (homepage script-signature detection and resilient persistence)
---

## 2026-02-23 00:13 UTC - S-8: Daily runner script
- Implemented `run_daily.py` orchestrator that runs all scraper modules sequentially with failure isolation and guaranteed exit code 0.
- Added daily log file wiring to `logs/daily_YYYY-MM-DD.log` plus console logging.
- Added per-scraper summary reporting (`name,status,rows_inserted,duration_seconds`) and robust `rows_inserted` calculation via pre/post table row counts.
- Added tests in `tests/test_run_daily.py` for failure-tolerant sequencing, log-file creation, and summary output format.
- **Learnings:** When scraper contracts only return booleans, before/after table counts provide a stable cross-scraper metric for inserted-row summaries.
- **SKILLS_APPLIED:** karpathy-coding (Python orchestration with fault-tolerant sequential execution, logging, and row-count metrics)
---

## 2026-02-23 00:24 UTC - S-9: Dashboard backend API
- Implemented `dashboard/server.py` as a Flask server with `create_app(db_path)` factory and `python dashboard/server.py` entrypoint bound to port 3001.
- Added REST JSON endpoints: `/api/prices`, `/api/products`, `/api/reviews`, `/api/diffs`, and `/api/ab-tests`, plus `/health`.
- Implemented shared competitor/date filtering (`competitor`, `date`, `start_date`, `end_date`) across all API routes.
- Added review-query compatibility logic that supports both schema variants (`total_reviews`/`rating_*` and migrated `review_count`/`stars_*`) without crashing.
- Added graceful database error handling with structured logging and JSON error responses.
- Added tests in `tests/test_dashboard_server.py` for empty DB behavior, endpoint payload correctness, and filtering behavior.
- **Learnings:** Using a dedicated data-store layer for API queries keeps endpoint handlers thin and ensures filtering logic stays consistent across all data types.
- **SKILLS_APPLIED:** karpathy-coding (Flask API design, shared query/filter abstractions, schema-compatible SQL, focused tests)
---

## 2026-02-23 00:30 UTC - S-10: Dashboard frontend — pricing view
- Implemented `dashboard/static/index.html` as a complete vanilla-JS single-page dashboard.
- Pricing tab includes: competitor/date-range/product filters, summary chips, pricing table with color-coded price badges, Chart.js 4 line trend chart per competitor, and a competitor × product comparison matrix.
- Loading spinner, empty, and error states for all components.
- Responsive layout with CSS media queries; accessibility with aria-label and role attributes.
- Added `/` route to `dashboard/server.py` (serves `index.html` via `send_from_directory`).
- Added `/api/competitors` endpoint and `fetch_competitors()` store method.
- Discovered: competitors table has `id`, `domain`, `base_url`, `created_at` — no `name` column.
- Tests in `tests/test_dashboard_frontend.py`: 36 tests covering route (200/HTML), structural HTML elements, /api/competitors content, and pricing API field compatibility.
- **Learnings:** Flask's default `static_folder` path is relative to the module file — use `Path(__file__).parent / "static"` with `send_from_directory` to serve index.html at `/`. The competitors table schema uses `domain` + `base_url` (no separate `name` field).
- **SKILLS_APPLIED:** karpathy-coding (surgical server changes, thin route handlers, store method, focused tests matching frontend contract)
---

## 2026-02-23 - S-10 (retry): Dashboard frontend — pricing view (typecheck fix)
- Fixed mypy `no-any-return` error in `tests/test_dashboard_frontend.py:_html()`
- Root cause: `FlaskClient.get().data` is typed as `Any` in Flask stubs; adding explicit `data: bytes` annotation lets mypy infer `.decode()` → `str`
- Change: one-line annotation `data: bytes = client.get("/").data` before the return
- **Learnings:** When Flask stubs expose `.data` as `Any`, always annotate the intermediate variable as `bytes` before calling `.decode()`; avoids `no-any-return` warnings throughout test helpers
---

## 2026-02-23 00:42 UTC - S-11: Dashboard frontend — reviews and products views
- Implemented full Reviews tab: Trustpilot/Google source pill-tabs, rating trend chart, review-count trend chart, latest ratings table, date + competitor filters
- Implemented full Products tab: competitor × product comparison matrix (✓/✗ with product-type badge), full catalog table with bundle badge, date + competitor filters
- Implemented full Diffs tab: per-diff expandable viewer with syntax-highlighted unified diff (green additions, red removals, blue headers), date + competitor filters
- Replaced placeholder stub tabs with real JS modules (loadAndRenderReviews, loadAndRenderProducts, loadAndRenderDiffs)
- Tab switching lazy-loads data on first visit; shared Chart.js helper + chart registry avoids canvas reuse leaks
- Files changed: dashboard/static/index.html (replaced + extended), tests/test_dashboard_s11.py (new, 66 tests)
- **SKILLS_APPLIED:** karpathy-coding (surgical frontend additions, shared Chart.js registry pattern, focused 66-test suite covering all new views)
- **Learnings:** Base schema uses `total_reviews` (not `review_count`) and `page_url` (not `url`) in snapshots — always check PRAGMA table_info before writing test fixtures; Chart.js canvases must be destroyed before re-creation (charts registry pattern works reliably)
---
